#!/usr/bin/env python
# coding=utf-8

"""Basic functions for binary classification performance measurement.

This module contains functions for elementary performance measurement of
**binary classifiers**.  Most of the functions rely on the input of system
output scores for both target (a.k.a.  positives) and non-target (a.k.a.
negatives) samples or integer counts (resulted from pre-thresholding output
scores).

We consider ``positives`` scores those originating from samples that are
labelled to belong to the class of interest (a.k.a., "signal" or "client").
``negatives`` scores represent system output for input samples that are labeled
**not** to belong to the class of interest (a.k.a., "noise" or "impostor").  It
is expected that "positive" scores are, at least by design and considering the
intrinsic score scale, typically greater than "negative" scores in a
well-designed classifier.

Every "positive" value that falls bellow the threshold is considered a
false-negative (FN) or a false-rejection (FR).  Analogously, "negative" samples
for which the output score of the system falls above the threshold are
considered a false-positive (FP) or false-acceptance (FA).  Positive scores
that fall on the threshold (exactly) are considered correctly classified
(a.k.a. "true positives").  Negatives that fall on the threshold (exactly) are
considered **incorrectly** classified (a.k.a. "false positives").

The "threshold" use to further classify positive and negative scores into true
and false variants does not necessarily have to fall within the range covered
by these scores (negatives and positives altogether).

In practical situations, it is possible that scores are inverted in the
negative/positive sense.  For example, in some setups the designer may have
setup the system so "positive" samples have a smaller score than the "negative"
ones. In this case, make sure you normalize the scores so positive samples have
greater scores before feeding them into these methods, e.g., by multiplying all
scores by -1.
"""

import numbers
import numpy


def true_negatives(negatives, threshold):
    """Evaluates correctly classifed negatives in a set, based on a threshold

    This method returns an array composed of booleans that pin-point, which
    negatives where correctly classified for the given threshold


    Parameters
    ==========

    negatives : numpy.ndarray (1D, float)

        The scores for non-target objects, or generated by comparing objects of
        different classes

    threshold : float

        The threshold, for which scores should be considered to be correctly
        classified.  The threshold cannot be ``NaN`` (not-a-number).


    Returns
    =======

    classified : numpy.ndarray (1D, bool)

        The decision for each of the ``negatives``

    """
    assert not numpy.isnan(threshold)
    return numpy.asarray(negatives) < threshold


correctly_classified_negatives = true_negatives


def false_positives(negatives, threshold):
    """Evaluates incorrectly classifed negatives in a set, based on a threshold

    This method returns an array composed of booleans that pin-point, which
    negatives where incorrectly classified as positives for the given
    threshold.


    Parameters
    ==========

    negatives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        different classes.

    threshold : float

        The threshold, for which scores should be considered to be correctly
        classified.  Cannot be ``NaN`` (not-a-number).


    Returns
    =======

    classified : numpy.ndarray (1D, bool)

        The decision for each of the ``negative``

    """
    assert not numpy.isnan(threshold)
    return true_positives(negatives, threshold)


def true_positives(positives, threshold):
    """Evaluates correctly classifed positives in a set, based on a threshold

    This method returns an array composed of booleans that pin-point, which
    positives where correctly classified for the given threshold.

    The pseudo-code for this function is:


    Parameters
    ==========

    positives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        the same classe

    threshold : float

        The threshold, for which scores should be considered to be correctly
        classified.  The threshold cannot be ``NaN`` (not-a-number).


    Returns
    =======

    classified : numpy.ndarray (1D, bool)

        The decision for each of the ``positives``

    """
    assert not numpy.isnan(threshold)
    return numpy.asarray(positives) >= threshold


correctly_classified_positives = true_positives


def false_negatives(positives, threshold):
    """Evaluates incorrectly classifed positives in a set, based on a threshold

    This method returns an array composed of booleans that pin-point, which
    positives where incorrectly classified for the given threshold

    The pseudo-code for this function is:


    Parameters
    ==========

    positives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        the same class

    threshold : float

        The threshold, for which scores should be considered to be correctly
        classified.  The threshold cannot be ``NaN`` (not-a-number).


    Returns
    =======

    classified : numpy.ndarray (1D, bool)

        The decision (if incorrectly classified) for each of the ``positives``.

    """
    assert not numpy.isnan(threshold)
    return true_negatives(positives, threshold)


def _tricky_division(n, d):
    """Divides n by d.  Returns 0.0 in case of a division by zero"""

    return n / (d + (d == 0))


def _rate_from_ints(*args):
    """Intermediate function to calculate rates"""
    if isinstance(args[0], numbers.Integral):
        return _tricky_division(args[0], args[1])
    # Sequence[float], float, function
    return _tricky_division(args[2](args[0], args[1]).sum(), len(args[0]))


def true_negative_rate(*args):
    """The true-negative or rejection rate, specificity, or selectivity

    By definition, the true-negative rate, specificity, or selectivity is the
    ratio between the number of true negatives and the total number of
    negatives.  If the number of negative scores is zero, this function
    exceptionally returns 0.0 instead of ``NaN``.

    This function accepts 2 signatures:

    * ``true_negative_rate(tn: int, ttn: int) -> float``
    * ``true_negative_rate(negatives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tn, ttn: int

        The number of :py:func:`true_negatives` and total negatives,
        pre-calculated.

    negatives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        different classes

    threshold : float

        The threshold to separate correctly and incorrectly classified scores.
        Cannot be ``NaN`` (not-a-number).


    Returns
    =======

    tnr : float

        The True Negative Rate (tnr) for the given threshold

    """
    # return tn / ttn
    return _rate_from_ints(args[0], args[1], true_negatives)


specificity = true_negative_rate
selectivity = true_negative_rate
true_rejection_rate = true_negative_rate


def false_positive_rate(*args):
    """The false-positive or acceptance rate, or fall-out

    By definition, the false-positive rate or fall-out is the ratio between the
    number of false positives and the total number of negatives.  If the number
    of negative scores is zero, this function exceptionally returns 0.0 instead
    of ``NaN``.

    This function accepts 2 signatures:

    * ``false_positive_rate(fp: int, ttn: int) -> float``
    * ``false_positive_rate(negatives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    fp, ttn: int

        The number of :py:func:`false_positives` and total negatives,
        pre-calculated.

    negatives : numpy.ndarray (1D, float)

        The scores for non-target objects, or generated by comparing objects of
        different classes

    threshold : float

        The threshold to separate correctly and incorrectly classified scores.
        Cannot be ``NaN`` (not-a-number).


    Returns
    =======

    fpr : float

        The False Positve Rate (FPR) for the given threshold.  If the number of
        negative scores is zero, this function exceptionally returns 0.0
        instead of ``NaN``.

    """
    # return fp / ttn
    return _rate_from_ints(args[0], args[1], false_positives)


fall_out = false_positive_rate
false_acceptance_rate = false_positive_rate


def true_positive_rate(*args):
    """The true-positive or acceptance rate, sensitivity, recall or hit rate

    By definition, the true-positive rate, sensitivity, recall or hit-rate is
    the ratio between the number of true positives and the total number of
    positives.  If the number of positives is zero, this function exceptionally
    returns 0.0 instead of ``NaN``.

    This function accepts 2 signatures:

    * ``true_positive_rate(tp: int, ttp: int) -> float``
    * ``true_positive_rate(positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tp, ttp: int

        The number of :py:func:`true_positives` and total number of positive
        scores, pre-calculated.

    positives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        the same class

    threshold : float

        The threshold to separate correctly and incorrectly classified scores.
        Cannot be ``NaN`` (not-a-number).


    Returns
    =======

    tpr : float

        The True Positive Rate (tpr) for the given threshold.  If the number of
        positives is zero, this function exceptionally returns 0.0 instead of
        ``NaN``.

    """
    # return tp / ttp
    return _rate_from_ints(args[0], args[1], true_positives)


sensitivity = true_positive_rate
recall = true_positive_rate
hit_rate = true_positive_rate
true_acceptance_rate = true_positive_rate


def false_negative_rate(*args):
    """The false-negative or rejection rate or miss-rate

    By definition, the false-negative rate or miss-rate is the ratio between
    the number of false negatives and the total number of positives. If the
    number of positives is zero, this function exceptionally returns 0.0
    instead of ``NaN``.

    This function accepts 2 signatures:

    * ``false_negative_rate(fn: int, ttp: int) -> float``
    * ``false_negative_rate(positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    fn, ttp: int

        The number of :py:func:`false_negatives` and total positives,
        pre-calculated.

    positives : numpy.ndarray (1D, float)

        The scores for target objects, or generated by comparing objects of
        the same class

    threshold : float

        The threshold to separate correctly and incorrectly classified scores.
        The threshold cannot be ``NaN`` (not-a-number).


    Returns
    =======

    fnr : float

        The False Negative Rate (FPR) for the given threshold.  If the number
        of positives is zero, this function exceptionally returns 0.0 instead
        of ``NaN``.

    """
    # return fn / ttp
    return _rate_from_ints(args[0], args[1], false_negatives)


miss_rate = false_negative_rate
false_rejection_rate = false_negative_rate


def precision(*args):
    r"""Calculates the precision or positive-predictive value (PPV)

    The precision or positive predictive value (PPV) is defined by:

    .. math::

       \\mathrm{ppv} = \\frac{tp}{tp + fp}

    Notice that, in the case ``tp+fp == 0``, this function returns 0.0,
    exceptionally.  TP and FP may be calculated from both ``negative`` and
    ``positive`` scores if a threshold is also provided.  This function has two
    signatures:

    * ``precision(fp: int, tp: int) -> float``
    * ``precision(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    fp, tp : int

        The number of :py:func:`false_positives` and :py:func:`true_positives`,
        pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The set of negative and positive scores to compute the measurement

    threshold : float

        The threshold to compute the measures for.  Cannot be ``NaN``.  Only
        useful if arrays of ``negatives`` and ``positives`` are provided.


    Returns
    =======

    precision : float

        The precision.  Notice that, in the case ``tn+fn == 0``, this function
        returns 0.0, exceptionally.

    """
    if isinstance(args[0], numbers.Integral):
        # return tp / (fp + tp)
        return _tricky_division(args[1], args[0] + args[1])

    return precision(
        false_positives(args[0], args[2]).sum(),  # fp
        true_positives(args[1], args[2]).sum(),  # tp
    )


positive_predictive_value = precision


def negative_predictive_value(*args):
    r"""Calculates the negative-predictive value (NPV)

    The negative predictive value (NPV) is defined by:

    .. math::

       \mathrm{npv} = \frac{tn}{tn + fn}

    Notice that, in the case ``tn+fn == 0``, this function returns 0.0,
    exceptionally.  TP and FP may be calculated from both ``negative`` and
    ``positive`` scores if a threshold is also provided.  This function has two
    signatures:

    * ``negative_predictive_value(tn: int, fn: int) -> float``
    * ``negative_predictive_value(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tn, fn : int

        The number of :py:func:`true_negatives` and :py:func:`false_negatives`,
        pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The set of negative and positive scores to compute the measurement

    threshold : float

        The threshold to compute the measures for.  Cannot be ``NaN``.  Only
        useful if arrays of ``negatives`` and ``positives`` are provided.


    Returns
    =======

    npv : float

        The negative-predictive value for the given negatives and positives, at
        a given threshold.  Notice that, in the case ``tn+fn == 0``, this
        function returns 0.0, exceptionally.


    """
    if isinstance(args[0], numbers.Integral):
        # return tn / (tn + fn)
        return _tricky_division(args[0], args[0] + args[1])

    return negative_predictive_value(
        true_negatives(args[0], args[2]).sum(),  # tn
        false_negatives(args[1], args[2]).sum(),  # fn
    )


def precision_recall(*args):
    """Handle to calculate precision and recall in a single call.

    See:

    * :py:func:`precision`
    * :py:func:`recall`

    This function has two signatures:

    * ``precision_recall(fp: int, tp: int, ttp: int) -> (float, float)``
    * ``precision_recall(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> (float, float)``


    Parameters
    ==========

    fp, tp, ttp: int

        The number of :py:func:`false_positives`, :py:func:`true_positives`,
        and total number of positive scores, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The set of negative and positive scores to compute the measurements

    threshold : float

        The threshold to compute the measures for


    Returns
    =======

    precision : float

        The precision value for the given negatives and positives

    recall : float

        The recall value for the given negatives and positives

    """
    if isinstance(args[0], numbers.Integral):
        return precision(args[0], args[1]), recall(args[1], args[2])
    elif not len(args[0]) or not len(args[1]):
        raise RuntimeError("negatives or positives should not be empty")
    return precision_recall(
        false_positives(args[0], args[2]).sum(),  # fp
        true_positives(args[1], args[2]).sum(),  # tp
        len(args[1]),  # ttp
    )


def fprfnr(*args):
    """Helper to calculate both the FPR (FAR) and FNR (FRR) in one go

    See:

    * :py:func:`false_positive_rate`
    * :py:func:`false_negative_rate`


    This function has two signatures:

    * ``fprfnr(fp: int, ttn: int, fn: int, ttp: int) -> (float, float)``
    * ``fprfnr(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> (float, float)``


    Parameters
    ==========

    fp, ttn, fn, ttp: int

        The number of :py:func:`false_positives`, total negatives,
        :py:func:`false_negatives`, and total positives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to separate correctly and incorrectly classified scores


    Returns
    =======

    fpr : float

        The False Positve Rate (FPR) or False Acceptance Rate (FAR) for the
        given threshold

    fnr : float

        The False Negative Rate (FNR) or False Reject Rate (FRR) for the given
        threshold

    """
    if isinstance(args[0], numbers.Integral):
        # return fp / ttn, fn / ttp
        return false_positive_rate(args[0], args[1]), false_negative_rate(
            args[2], args[3]
        )
    elif not len(args[0]) or not len(args[1]):
        raise RuntimeError("negatives or positives should not be empty")
    return fprfnr(
        false_positives(args[0], args[2]).sum(),  # fp
        len(args[0]),  # ttn
        false_negatives(args[1], args[2]).sum(),  # fn
        len(args[1]),  # ttp
    )


farfrr = fprfnr


def f1_score(*args):
    r"""Computes the F-score of the accuracy of the classification

    The F-score is a weighted mean of precision and recall measurements, see
    :py:func:`precision_recall`.  It is computed as:

    .. math::

       \mathrm{\text{f-score}}(w) = (1 + w^2)\frac{\mathrm{precision}\cdot{}\mathrm{recall}}{w^2\cdot{}\mathrm{precision} + \mathrm{recall}}

    The weight :math:`w` needs to be non-negative real value. In case the
    weight parameter is 1 (current implementation), the F-score is called F1
    score and represents the harmonic mean between precision and recall values.
    This function exceptionally returns ``0.0`` if the denominator above is
    zero.

    This function has two signatures:

    * ``f1_score(fp: int, tp: int, ttn: int) -> float``
    * ``f1_score(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    fp, tp, ttn: int

        The number of :py:func:`false_positives`, :py:func:`true_positives`,
        and total negatives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the f1-score for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    =======

    f1_score : float

        The computed F1-score

    """
    p, r = precision_recall(*args)
    return _tricky_division(2 * (p * r), (p + r))


def accuracy(*args):
    r"""Computes the the accuracy of classification

    Accuracy is the proportion of correct predictions (both true positives and
    true negatives) among the total number of elements examined.  It
    corresponds arithmetically to:

    .. math::
       :nowrap:

       \begin{align*}
           \mathrm{Acc} &= \frac{\mathrm{tn}+\mathrm{tp}}{\mathrm{tn}+\mathrm{fp}+\mathrm{tp}+\mathrm{fn}} \\
           \mathrm{Acc} &= \frac{\mathrm{tn}+\mathrm{tp}}{\mathrm{ttn}+\mathrm{ttp}}
       \end{align*}

    In the special case where ``ttn+ttp == 0``, this function returns zero for
    accuracy.

    Because it considers both TP and TN in the numerator, this measure is
    sensitive to threshold domains which may be completely dominated by true
    positive or negative scores.

    This function has two signatures:

    * ``accuracy(tn: int, ttn: int, tp: int, ttp: int) -> float``
    * ``accuracy(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tn, ttn, tp, ttp: int

        The number of :py:func:`true_negatives`, total negatives,
        :py:func:`true_positives`, total positives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the accuracy for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    =======

    acc : float

        The computed accuracy

    """
    if isinstance(args[0], numbers.Integral):
        # (tn+tp)/(ttn+ttp)
        return tricky_division(args[0] + args[2], args[1] + args[3])
    return accuracy(
        true_negatives(args[0], args[2]).sum(),  # tn
        len(args[0]),  # ttn
        true_positives(args[1], args[2]).sum(),  # tp
        len(args[1]),  # ttp
    )


def balanced_accuracy(*args):
    r"""Computes the the balanced accuracy of classification

    Balance accuracy corresponds arithmetically to:

    .. math::

       \mathrm{BA} = \frac{\mathrm{tnr}+\mathrm{tpr}}{2}

    This function has two signatures:

    * ``balanced_accuracy(tn: int, ttn: int, tp: int, ttp: int) -> float``
    * ``balanced_accuracy(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tn, ttn, tp, ttp: int

        The number of :py:func:`true_negatives`, total negatives,
        :py:func:`true_positives`, total positives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the balanced accuracy for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    =======

    ba : float

        The computed balanced accuracy

    """
    if isinstance(args[0], numbers.Integral):
        return (true_negative_rate(tn, ttn) + true_positive_rate(tp, ttp)) / 2.0
    return balanced_accuracy(
        true_negatives(args[0], args[2]).sum(),  # tn
        len(args[0]),  # ttn
        true_positives(args[1], args[2]).sum(),  # tp
        len(args[1]),  # ttp
    )


def half_total_error_rate(*args):
    r"""Computes the the half-total error rate (HTER) of classification

    The HTER corresponds arithmetically to:

    .. math::
       :nowrap:

       \begin{align*}
           \mathrm{HTER} &= \frac{\mathrm{fpr}+\mathrm{fnr}}{2} \\
           \mathrm{HTER} &= \frac{(1-\mathrm{tnr})+(1-\mathrm{tpr})}{2} \\
           \mathrm{HTER} &= 1 - \frac{\mathrm{tnr}+\mathrm{tpr}}{2} \\
           \mathrm{HTER} &= 1 - BA
       \end{align*}

    * See :py:func:`balanced_accuracy`

    This function has two signatures:

    * ``half_total_error_rate(tn: int, ttn: int, tp: int, ttp: int) -> float``
    * ``half_total_error_rate(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> float``


    Parameters
    ==========

    tn, ttn, tp, ttp: int

        The number of :py:func:`true_negatives`, total negatives,
        :py:func:`true_positives`, total positives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the half-total error rate for.  Cannot be
        ``NaN`` (not-a-number).


    Returns
    =======

    hter : float

        The computed half-total error rate

    """
    return 1.0 - balanced_accuracy(*args)


def jaccard_index(*args):
    r"""Computes the Jaccard or Similarity index of a classifier

    The Jaccard-index corresponds arithmetically:

    .. math::
       :nowrap:

       \begin{align*}
           \mathrm{J} &= \frac{\mathrm{tp}}{\mathrm{fp}+\mathrm{tp}+\mathrm{fn}} \\
           \mathrm{J} &= \frac{\mathrm{tp}}{\mathrm{fp}+\mathrm{ttp}}
       \end{align*}

    In the special case where ``tn+fp+fn == 0``, this function returns zero for
    the Jaccard index.

    The Jaccard index depends on a TP-only numerator, similarly to the F1
    score.  This measure is not sensible to regions of the score domain where
    no true-positives are available.  The ``accuracy`` may be a better proxy in
    these cases.

    This function has two signatures:

    * ``jaccard_index(fp, tp, ttp)``
    * ``jaccard_index(negatives, positives, threshold)``


    Parameters
    ==========

    fp, tp, ttp: int

        The number of :py:func:`false_positives`, :py:func:`true_positives` and
        total positive scores, pre-calculated.

    negatives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the Jaccard index for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    =======

    jaccard_index : float

        The computed Jaccard or Similarity index for the given scores and the
        given threshold

    """
    if isinstance(args[0], numbers.Integral):
        # tp/(fp+ttp)
        return tricky_division(args[1], args[0] + args[2])
    return jaccard_index(
        false_positives(args[0], args[2]).sum(),  # fp
        true_positives(args[1], args[2]).sum(),  # tp
        len(args[1]),  # ttp
    )


jaccard = jaccard_index
similarly_index = jaccard_index


def matthews_correlation_coefficient(*args):
    r"""Computes the Matthews Correlation Coefficient (MCC) or Phi Coefficient of a classifier

    The MCC corresponds arithmetically:

    .. math::

       \mathrm{MCC} = \frac{\mathrm{tp}\mathrm{tn} - \mathrm{fp}\mathrm{fn}}{\sqrt{(\mathrm{tp}+\mathrm{fp})(\mathrm{tp}+\mathrm{fn})(\mathrm{tn}+\mathrm{fp})(\mathrm{tn}+\mathrm{fn})}}

    In the special case where the denominator is ``0.0``, this function returns
    zero for the MCC.

    This function has two signatures:

    * ``matthews_correlation_coefficient(tn: int, ttn: int, tp: int, ttp: int) -> float``
    * ``matthews_correlation_coefficient(negatives: Sequence[float], positives: Sequence[float], threshold: Sequence[float]) -> float``


    Parameters
    ==========

    tn, ttn, tp, ttp: int

        The number of :py:func:`true_negatives`, total negative scores,
        :py:func:`true_positives` and total positive scores, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the MCC for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    =======

    mcc : float

        The computed MCC of Phi Coefficient

    """
    if isinstance(args[0], numbers.Integral):
        # ((tp*tn) - (fp*fn)/sqrt((tp+fp)(tp+fn)(tn+fp)(tn+fn))
        tn = args[0]
        fp = args[1] - args[0]
        tp = args[2]
        fn = args[3] - args[2]
        return tricky_division(
            (tp * tn) - (fp * fn),
            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5,
        )
    return matthews_correlation_coefficient(
        true_negatives(args[0], args[2]).sum(),  # tn
        len(negatives),  # ttn
        true_positives(args[1], args[2]).sum(),  # tp
        len(positives),  # ttp
    )


phi_coefficient = matthews_correlation_coefficient


def measures(*args):
    """In a single call, calculate most of the quantities in this module

    This function simplifies the calculation of most performance measures in
    this module through a single call.

    This function accepts 2 signatures:

    * ``measures(tn: int, ttn: int, tp: int, ttp: int) -> Sequence[float]``
    * ``measures(negatives: Sequence[float], positives: Sequence[float], threshold: float) -> Sequence[float]``


    Parameters
    ----------

    tn, ttn, tp, ttp: int

        The number of :py:func:`true_negatives`, total negatives,
        :py:func:`true_positives`, total positives, pre-calculated.

    negatives, positives : numpy.ndarray (1D, float)

        The scores for non-target and target samples

    threshold : float

        The threshold to compute the accuracy for.  Cannot be ``NaN``
        (not-a-number).


    Returns
    -------

    specificity : float
        Selectivity or true negative rate, see :py:func:`specificity`

    recall : float
        Sensitivity, hit rate, or true positive rate, see :py:func:`recall`

    precision : float
        Or Positive-Predictive Value (PPV), see :py:func:`precision`

    f1_score : float
        See :py:func:`f1_score`

    accuracy : float
        See :py:func:`accuracy`

    half_total_error_rate : float
        See :py:func:`half_total_error_rate`

    jaccard_index : float
        See :py:func:`jaccard_index`

    matthews_correlation_coefficient : float
        See :py:func:`matthews_correlation_coefficient`

    """
    if isinstance(args[0], numbers.Integral):
        tn = args[0]
        ttn = args[1]
        fp = ttn - tn
        tp = args[2]
        ttp = args[3]
        return (
            specificity(tn, ttn),  # ==true negative rate
            recall(tp, ttp),  # == true positive rate
            precision(fp, tp),  # == positive predicitve value
            f1_score(fp, tp, ttn),
            accuracy(tn, ttn, tp, ttp),
            balanced_accuracy(tn, ttn, tp, ttp),
            half_total_error_rate(tn, ttn, tp, ttp),
            jaccard_index(fp, tp, ttp),
            matthews_correlation_coefficient(tn, ttn, tp, ttp),
        )

    neg = args[0]
    pos = args[1]
    t = args[2]
    return (
        specificity(neg, t),
        recall(pos, t),
        precision(neg, pos, t),
        f1_score(neg, pos, t),
        accuracy(neg, pos, t),
        balanced_accuracy(neg, pos, t),
        half_total_error_rate(neg, pos, t),
        jaccard_index(neg, pos, t),
        matthews_correlation_coefficient(neg, pos, t),
    )


def eer(negatives, positives, is_sorted=False, also_farfrr=False):
    """Calculates the Equal Error Rate (EER)

    Please note that it is possible that eer != fpr != fnr.  This function
    returns (fpr + fnr) / 2 as eer.  If you also need the fpr and fnr values,
    set ``also_farfrr`` to ``True``.


    Parameters
    ==========

    negatives, positives : numpy.ndarray (1D, float)
        The set of negative and positive scores to compute the measurement

    is_sorted : bool
        Are both sets of scores already in ascendantly sorted order?

    also_farfrr : bool
        If True, it will also return far and frr.


    Returns
    =======

    eer : float
        The Equal Error Rate (EER).

    fpr : float
        The False Positive Rate (FPR). Returned only when ``also_farfrr`` is
        ``True``.

    fnr : float
        The False Negative Rate (FNR). Returned only when ``also_farfrr`` is
        ``True``.

    """
    from brute_force import eer_threshold

    threshold = eer_threshold(negatives, positives, is_sorted)
    far, frr = farfrr(negatives, positives, threshold)
    if also_farfrr:
        return (far + frr) / 2.0, far, frr
    return (far + frr) / 2.0
