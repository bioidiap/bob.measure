<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Credible and Confidence Intervals &mdash; bob.measure 4.2.4b0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="_static/plot_directive.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Python API" href="api.html" />
    <link rel="prev" title="User Guide" href="guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> bob.measure
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                4.2.4b0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="guide.html">User Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Credible and Confidence Intervals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#credible-interval-or-region">Credible Interval (or Region)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applicability-to-figures-of-merit-in-classification">Applicability to Figures of Merit in Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#confidence-interval">Confidence Interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conservativeness">Conservativeness</a></li>
<li class="toctree-l2"><a class="reference internal" href="#comparing-2-systems">Comparing 2 systems</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#unpaired-comparison">Unpaired comparison</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#comparing-f1-scores">Comparing F1-Scores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#roc-and-precision-recall-curves">ROC and Precision Recall Curves</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#paired-comparison">Paired comparison</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#q-a">Q&amp;A</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#i-m-confused-what-should-i-choose">I’m confused, what should I choose?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-if-my-sampling-is-not-i-i-d">What if my sampling is not i.i.d.?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#should-i-consider-the-expected-value-or-mode-of-a-credible-interval">Should I consider the expected value or mode of a credible interval?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#is-there-a-better-way-to-compare-2-systems">Is there a better way to compare 2 systems?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">bob.measure</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Credible and Confidence Intervals</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/ci.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="credible-and-confidence-intervals">
<span id="bob-measure-ci"></span><h1>Credible and Confidence Intervals<a class="headerlink" href="#credible-and-confidence-intervals" title="Permalink to this headline">¶</a></h1>
<section id="credible-interval-or-region">
<span id="bob-measure-ci-credible"></span><h2>Credible Interval (or Region)<a class="headerlink" href="#credible-interval-or-region" title="Permalink to this headline">¶</a></h2>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Credible_interval">Credible Interval</a> or region (for multi-dimensional
cases) for parameter <span class="math notranslate nohighlight">\(x\)</span> consists of a lower estimate <span class="math notranslate nohighlight">\(L\)</span>, and an
upper estimate <span class="math notranslate nohighlight">\(U\)</span>, such that the probability of the true value being
within the interval estimate is equal to <span class="math notranslate nohighlight">\(\alpha\)</span>.  For example, a 95%
credible interval (i.e.  <span class="math notranslate nohighlight">\(\alpha = 0.95\)</span>) for a parameter <span class="math notranslate nohighlight">\(x\)</span> is
given by <span class="math notranslate nohighlight">\([L, U]\)</span> such that</p>
<div class="math notranslate nohighlight">
\[P(k \in [L,U]) = 95%\]</div>
<p>The smaller the test size, the wider the confidence interval will be, and the
greater <span class="math notranslate nohighlight">\(\alpha\)</span>, the smaller the confidence interval will be.  The
evaluation of credible intervals follows a bayesian approach, where one assumes
a prior probability density function that models the likelihood of the
parameter, given its possible range.  Once the prior function is established,
the Bayes theorem is used to devise the posterior distribution of the
parameter given its current estimate, and to calculate the credible interval.</p>
<section id="background">
<h3>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h3>
<p>In binary classification problems <strong>where each sample is i.i.d.</strong> (independent
and identically distributed random variables), success/failure tests are
binomially distributed (that is, composed of a number <span class="math notranslate nohighlight">\(n\)</span> of Bernoulli
trials using a biased coin with “unknown” proportion <span class="math notranslate nohighlight">\(p\)</span>):</p>
<div class="math notranslate nohighlight">
\[P (K = k\mid p) = \binom{n}{k} p^k (1-p)^{n-k}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the total number of trials for a particular experiment,</p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> is the number of successes within those trials, and</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is the true probabibility, which is unknown, and that we are trying
to estimate from experiments.</p></li>
</ul>
<p>To estimate <span class="math notranslate nohighlight">\(p\)</span>, the true value for the unknow probability, given
<span class="math notranslate nohighlight">\(k\)</span>, the number of successes observed in a concrete experiment, we apply
the Bayes theorem:</p>
<div class="math notranslate nohighlight">
\[P(p\mid k) = \frac{P(k\mid p) P(p)}{P(k)}\]</div>
<p>The value of <span class="math notranslate nohighlight">\(P(k)\)</span> does not depend on <span class="math notranslate nohighlight">\(n\)</span> and can be recast as a
marginalized version of <span class="math notranslate nohighlight">\(P(k\mid p)\)</span>, which will help us later:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(p\mid k) &amp;= \frac{P(k\mid p) P(p)} {P(k)} \\
           &amp; = \frac{P(k\mid p) P(p)} {\int_{p'} P(k, p') dp'} \\
           &amp; = \frac{P(k\mid p) P(p)} {\int_{p'} P(k\mid p') P(p') dp'}\end{split}\]</div>
<p>In the case of a binomial distribution, <span class="math notranslate nohighlight">\(P(k\mid p)\)</span> is known (first
equation).  <span class="math notranslate nohighlight">\(P(k)\)</span> is normally called the <em>prior</em> probability density,
and corresponds to a known (or most likely) density distribution for the
parameter <span class="math notranslate nohighlight">\(k\)</span>.  The choice of this prior will of course affect the
overall aspect of the posterior distribution <span class="math notranslate nohighlight">\(P(p\mid k)\)</span> we are trying
to estimate.</p>
<p>A typical choice for this prior is a <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>.  As it turns out, a
Beta prior will generate a (conjugate) Beta posterior:</p>
<div class="math notranslate nohighlight">
\[P(p\mid k) = \frac{1}{B(k+\alpha,n-k+\beta} p^{k+\alpha-1}(1-p)^{n-k+\beta-1}\]</div>
<p>This formulation provides us with a complete representation for the posterior
of <span class="math notranslate nohighlight">\(p\)</span>, allowing the calculation of credible intervals, via integration.
The values <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are hyper-parameters which control
the skewness of the Beta distribution towards the maximum or the minimum
respectively.  As there is no reason to favour one more than the other, these
are typically set to matching values <span class="math notranslate nohighlight">\(\alpha=\beta=\lambda\)</span>.</p>
<div class="math notranslate nohighlight">
\[P(p\mid k) = \frac{1}{B(k+\lambda,n-k+\lambda} p^{k+\lambda-1}(1-p)^{n-k+\lambda-1}\]</div>
<p>Two classical settings are often used:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = 1\)</span> (a.k.a. a “flat” prior)</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda = 0.5\)</span> (a.k.a. Jeoffrey’s prior)</p></li>
</ul>
<p>In practice, changing <span class="math notranslate nohighlight">\(\lambda\)</span> does not affect much the credible
interval calculation.  To calculate the credible interval for a binary variable
using a flat or Jeoffrey’s prior, use
<a class="reference internal" href="api/bob.measure.credible_region.html#bob.measure.credible_region.beta" title="bob.measure.credible_region.beta"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.credible_region.beta()</span></code></a>, providing <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(l=(n-k)\)</span>, <span class="math notranslate nohighlight">\(\lambda\)</span> and how much coverage you would like to have
(typically 0.95 - 95%).</p>
</section>
<section id="applicability-to-figures-of-merit-in-classification">
<span id="bob-measure-ci-figures"></span><h3>Applicability to Figures of Merit in Classification<a class="headerlink" href="#applicability-to-figures-of-merit-in-classification" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="references.html#goutte-2005" id="id1"><span>[GOUTTE-2005]</span></a> extended this analysis to classifical figures of merit in
classification, with a similar reasoning as used for accuracy on the previous
section:</p>
<ul class="simple">
<li><p>Precision or Positive-Predictive Value (PPV): <span class="math notranslate nohighlight">\(p = TP/(TP+FP)\)</span>, so
<span class="math notranslate nohighlight">\(k=TP\)</span>, <span class="math notranslate nohighlight">\(l=FP\)</span></p></li>
<li><p>Recall, Sensitivity, or True Positive Rate: <span class="math notranslate nohighlight">\(r = TP/(TP+FN)\)</span>, so
<span class="math notranslate nohighlight">\(k=TP\)</span>, <span class="math notranslate nohighlight">\(l=FN\)</span></p></li>
<li><p>Specificity or True Negative Rage: <span class="math notranslate nohighlight">\(s = TN/(TN+FP)\)</span>, so <span class="math notranslate nohighlight">\(k=TN\)</span>,
<span class="math notranslate nohighlight">\(l=FP\)</span></p></li>
<li><p>Accuracy: <span class="math notranslate nohighlight">\(acc = TP+TN/(TP+TN+FP+FN)\)</span>, so <span class="math notranslate nohighlight">\(k=TP+TN\)</span>,
<span class="math notranslate nohighlight">\(l=FP+FN\)</span></p></li>
<li><p>Jaccard Index: <span class="math notranslate nohighlight">\(j = TP/(TP+FP+FN)\)</span>, so <span class="math notranslate nohighlight">\(k=TP\)</span>, <span class="math notranslate nohighlight">\(l=FP+FN\)</span></p></li>
</ul>
<p>The function <a class="reference internal" href="api/bob.measure.credible_region.html#bob.measure.credible_region.measures" title="bob.measure.credible_region.measures"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.credible_region.measures()</span></code></a> can calculate the
above quantites in a single shot from counts of true and false, positives and
negatives, the <span class="math notranslate nohighlight">\(\lambda\)</span> parameter, and the desired coverage.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For comparing two systems according to their F1-score, please see
<a class="reference internal" href="#bob-measure-ci-compare"><span class="std std-ref">Comparing 2 systems</span></a>.</p>
</div>
</section>
</section>
<section id="confidence-interval">
<span id="bob-measure-ci-confidence"></span><h2>Confidence Interval<a class="headerlink" href="#confidence-interval" title="Permalink to this headline">¶</a></h2>
<p>A <a class="reference internal" href="#confidence-interval">confidence interval</a> corresponds to a <em>frequentist</em> approach to the
estimation of a range of values for an unknown parameter <span class="math notranslate nohighlight">\(p\)</span>.  More
formally, a 95% confidence interval means that with a large number of <cite>n</cite>
repeated Bernoulli trials, 95% of times, the estimated interval would include
the true value of the parameter.  Where as in the Bayesian approach the
interval is fixed and the parameter <span class="math notranslate nohighlight">\(p\)</span> is subject to random process, in
the frequentist approach, the parameter <span class="math notranslate nohighlight">\(p\)</span> is fixed and the interval is
subject to randomness.</p>
<p>There are several proposed ways to calculate a confidence interval, some of
which are implemented in this package.  They differ by the “conservativeness”,
or how large the interval will be for the same coverage.  Except for specific
method parameterization (<span class="math notranslate nohighlight">\(\lambda\)</span>), they should (almost) work as a
drop-in replacement for <a class="reference internal" href="api/bob.measure.credible_region.html#bob.measure.credible_region.beta" title="bob.measure.credible_region.beta"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.credible_region.beta()</span></code></a>:</p>
<ul class="simple">
<li><p>Wilson, 1927, <a class="reference internal" href="references.html#wilson-1927" id="id2"><span>[WILSON-1927]</span></a>:
<a class="reference internal" href="api/bob.measure.confidence_interval.html#bob.measure.confidence_interval.wilson" title="bob.measure.confidence_interval.wilson"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.confidence_interval.wilson()</span></code></a></p></li>
<li><p>Clopper-Pearson, 1934, <a class="reference internal" href="references.html#clopper-1934" id="id3"><span>[CLOPPER-1934]</span></a>:
<a class="reference internal" href="api/bob.measure.confidence_interval.html#bob.measure.confidence_interval.clopper_pearson" title="bob.measure.confidence_interval.clopper_pearson"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.confidence_interval.clopper_pearson()</span></code></a></p></li>
<li><p>Agresti-Coull, 1998, <a class="reference internal" href="references.html#agresti-1998" id="id4"><span>[AGRESTI-1998]</span></a>:
<a class="reference internal" href="api/bob.measure.confidence_interval.html#bob.measure.confidence_interval.agresti_coull" title="bob.measure.confidence_interval.agresti_coull"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.confidence_interval.agresti_coull()</span></code></a></p></li>
</ul>
</section>
<section id="conservativeness">
<span id="bob-measure-ci-conservativeness"></span><h2>Conservativeness<a class="headerlink" href="#conservativeness" title="Permalink to this headline">¶</a></h2>
<p>When talking about credible or confidence intervals, one the most important
aspect relates to the method conservativeness.  A method that is too
conservative (pessimistic) will tend to provide larger than required intervals
that surpass the required coveraged.  If you are using this to compare systems
(e.g. compare models A and B performance through the same database), then a
too-pessimistic approach may result in overlapping performance intervals for
both systems.  Therefore, it is preferrable to use a technique that is as
precise as possible.</p>
<p>Estimating conservativeness is a difficult task.  The main problem relates to
the underlying hypothesis samples are issued from a binomial distribution
(i.e., are true Bernoulli trials).  Considering that to be true, you can test
the various methods coverage regions through simulation.  The function
<a class="reference internal" href="api/bob.measure.curves.html#bob.measure.curves.estimated_ci_coverage" title="bob.measure.curves.estimated_ci_coverage"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.curves.estimated_ci_coverage()</span></code></a> can help you with that
task, and provides an usage example for the various intervals implemented in
this package:</p>
<p>(<a class="reference external" href="./examples/coverage.py">Source code</a>, <a class="reference external" href="./examples/coverage.png">png</a>, <a class="reference external" href="./examples/coverage.hires.png">hires.png</a>, <a class="reference external" href="./examples/coverage.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/coverage.png" class="plot-directive" src="_images/coverage.png" />
</figure>
</section>
<section id="comparing-2-systems">
<span id="bob-measure-ci-compare"></span><h2>Comparing 2 systems<a class="headerlink" href="#comparing-2-systems" title="Permalink to this headline">¶</a></h2>
<p>According to <a class="reference internal" href="references.html#goutte-2005" id="id5"><span>[GOUTTE-2005]</span></a>, 2 distincsystems may be compared by considering
either two specific use-cases: if both systems are exposed to the same (paired
comparison) or different data (unpaired).  As a rule of thumb, paired tests
carry stronger constraints and therefore have the potential provide more
sensitive credible intervals.</p>
<section id="unpaired-comparison">
<h3>Unpaired comparison<a class="headerlink" href="#unpaired-comparison" title="Permalink to this headline">¶</a></h3>
<p>To perform an unpaired comparison of two systems, we first assume each system
have a fixed threshold and therefore can produce two sets of true and false
positive and negative scores.  Under these conditions, one may compare two
figures of merit (<a class="reference internal" href="#bob-measure-ci-figures"><span class="std std-ref">Applicability to Figures of Merit in Classification</span></a>), taken as base their posterior
beta distributions.  This cannot be done analytically, so we resort to a <a class="reference external" href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte
Carlo simulation</a>.  In the following example, we compare the precision and
recall of two systems previously tuned.</p>
<p>(<a class="reference external" href="./examples/precision-comparison.py">Source code</a>, <a class="reference external" href="./examples/precision-comparison.png">png</a>, <a class="reference external" href="./examples/precision-comparison.hires.png">hires.png</a>, <a class="reference external" href="./examples/precision-comparison.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/precision-comparison.png" class="plot-directive" src="_images/precision-comparison.png" />
</figure>
<p>From this, we can assert that system’s 2 precision is only 65% (empirically)
likely to be better than system’s 1 precision.  A test for a 95% credible
interval, in this case, would fail.  Of course, we can apply the same reasoning
for the recall.</p>
<p>(<a class="reference external" href="./examples/recall-comparison.py">Source code</a>, <a class="reference external" href="./examples/recall-comparison.png">png</a>, <a class="reference external" href="./examples/recall-comparison.hires.png">hires.png</a>, <a class="reference external" href="./examples/recall-comparison.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/recall-comparison.png" class="plot-directive" src="_images/recall-comparison.png" />
</figure>
<p>Here, we see that there is only a 24% probability that system’s 2 recall is
better than system 1.  Again, a check for a 95% margin would fail.</p>
<section id="comparing-f1-scores">
<h4>Comparing F1-Scores<a class="headerlink" href="#comparing-f1-scores" title="Permalink to this headline">¶</a></h4>
<p>Comparing F1-scores cannot be done with the procedure above, using a Beta
posterior, according to <a class="reference internal" href="references.html#goutte-2005" id="id6"><span>[GOUTTE-2005]</span></a>.  We resort again to Monte Carlo
simulations for this.</p>
<p>(<a class="reference external" href="./examples/f1-comparison.py">Source code</a>, <a class="reference external" href="./examples/f1-comparison.png">png</a>, <a class="reference external" href="./examples/f1-comparison.hires.png">hires.png</a>, <a class="reference external" href="./examples/f1-comparison.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/f1-comparison.png" class="plot-directive" src="_images/f1-comparison.png" />
</figure>
<p>From this, we cannot assert that neither system’s 1 or 2 F1-score is greater
than the other one with a 95% confidence.</p>
</section>
<section id="roc-and-precision-recall-curves">
<h4>ROC and Precision Recall Curves<a class="headerlink" href="#roc-and-precision-recall-curves" title="Permalink to this headline">¶</a></h4>
<p>It is often useful to explore multiple thresholds at the same time, instead of
tuning each system to a set of thresholds.  This package allows you to plot ROC
and Precision-Recall curves including credible/confidence interval bounds.  We
simply repeat the procedure above for each threshold and plot the results.  The
bounds of the curve are calculated taking into consideration the credible
interval at that particular point, in all four directions.  The calculation of
the upper bound for a typical performance curve is examplified below:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/curve-estimation-algorithm.svg"><img alt="_images/curve-estimation-algorithm.svg" src="_images/curve-estimation-algorithm.svg" width="80%" /></a>
</figure>
<p>The lower bounds of a performance curve are calculated using the same
principle, using the lower ellypse formed by the lower bounds of the credible
interval.</p>
<p>Here is an example on how to use the built-in procedures for plotting ROC
curves:</p>
<p>(<a class="reference external" href="./examples/roc-ci.py">Source code</a>, <a class="reference external" href="./examples/roc-ci.png">png</a>, <a class="reference external" href="./examples/roc-ci.hires.png">hires.png</a>, <a class="reference external" href="./examples/roc-ci.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/roc-ci.png" class="plot-directive" src="_images/roc-ci.png" />
</figure>
<p>The plotting procedure produces the area under the ROC curve (AUROC) for all
three curves: the normal ROC, as well as the lower and upper bounds, which
allows for the estimation of the 95% credible interval of that measure.  In
this example, it is possible to observe both system performances overlap a bit,
from the perspective of the AUROC.  A more thorough analysis would require the
selection of a threshold for each system, and a more detailed CI analysis.</p>
<p>We can also create Precision vs. Recall plots.  Here is the above system from
that perspective.</p>
<p>(<a class="reference external" href="./examples/precision-recall-curve-ci.py">Source code</a>, <a class="reference external" href="./examples/precision-recall-curve-ci.png">png</a>, <a class="reference external" href="./examples/precision-recall-curve-ci.hires.png">hires.png</a>, <a class="reference external" href="./examples/precision-recall-curve-ci.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/precision-recall-curve-ci.png" class="plot-directive" src="_images/precision-recall-curve-ci.png" />
</figure>
<p>With the precision-recall curves, our stock canvas also includes iso-F1 lines,
which show equal-valued F1-score segments.  Also on this view, both systems
present significant overlap in terms of the area under the PR curve (AUPRC).</p>
</section>
</section>
<section id="paired-comparison">
<h3>Paired comparison<a class="headerlink" href="#paired-comparison" title="Permalink to this headline">¶</a></h3>
<p>The paired (binary) comparison described in <a class="reference internal" href="references.html#goutte-2005" id="id7"><span>[GOUTTE-2005]</span></a>, equations 16 and 19
is implemented.  It is an extension of the bayesian approach described for the
various figures of merit (precision, recall, specificity, etc), by considering
a Dirichlet posterior (generalization of the beta posterior for a multinomial
distribution), with the following parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_1\)</span>: then number of times system 1 gets the output right (matches
label), and system 2 gets it wrong</p></li>
<li><p><span class="math notranslate nohighlight">\(N_2\)</span>: then number of times system 2 gets the output right (matches
label), and system 1 gets it wrong</p></li>
<li><p><span class="math notranslate nohighlight">\(N_3\)</span>: then number of times system 1 and 2 outputs match, even if they
do not match the reference label</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In a paired comparison, both systems to be compared (systems 1 and 2) have
been previously tunned, and output only binary outcomes.  The binary
outcomes of each system are compared with the reference labels to determine
correct or incorrect hypotheses.</p>
</div>
<p>Notice <span class="math notranslate nohighlight">\(N = \sum_i N_i\)</span> corresponds to the total number of samples
available for the comparison.  By consequence, <span class="math notranslate nohighlight">\(pi_i = N_i / N\)</span>
corresponds to the multinomial probability one of the above outcomes take
place.  The implemented posterior has the following form:</p>
<div class="math notranslate nohighlight">
\[P(\pi|Z,\alpha) = \frac{\Gamma(N + \sum_k \alpha_k)}{\prod_k \Gamma(N_k + \alpha_k)} \prod_k \pi_k^{N_k + \alpha_k -1}\]</div>
<p>Each <cite>alpha_k</cite> corresponds to the prior information about the distribution of
each probability.  Typically, a symmetrical prior is used such as Jeoffrey’s
(<span class="math notranslate nohighlight">\(\alpha_k=0.5\)</span>) or flat (<span class="math notranslate nohighlight">\(\alpha_k=1\)</span>).  Given the various
relationships above, the implemented function inputs only <span class="math notranslate nohighlight">\(N_k\)</span> and
<span class="math notranslate nohighlight">\(\alpha_k\)</span>.  The outcome is computed via Monte-Carlo simulation, for
which you must decide on the number of samples.  Values in the order of
millions are typically used for a more precise estimation.</p>
<p>The following piece of code simulates two systems and runs a paired comparison
using the built-in function
<a class="reference internal" href="api/bob.measure.credible_region.html#bob.measure.credible_region.compare_systems" title="bob.measure.credible_region.compare_systems"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.credible_region.compare_systems()</span></code></a>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total number of samples: </span><span class="si">{</span><span class="n">nb_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio negatives/positives: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ratio</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;System 1 accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">system1_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;System 2 accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">system2_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="c1"># calculate when systems agree and disagree</span>
<span class="n">n1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span>
    <span class="p">(</span><span class="o">~</span><span class="n">numpy</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">system1_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>  <span class="c1"># correct for system 1</span>
    <span class="o">&amp;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">system2_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>   <span class="c1"># incorrect for system 2</span>
<span class="p">)</span>
<span class="n">n2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span>
    <span class="p">(</span><span class="o">~</span><span class="n">numpy</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">system2_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>  <span class="c1"># correct for system 2</span>
    <span class="o">&amp;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">system1_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>   <span class="c1"># incorrect for system 1</span>
<span class="p">)</span>
<span class="n">n3</span> <span class="o">=</span> <span class="n">nb_samples</span> <span class="o">-</span> <span class="n">n1</span> <span class="o">-</span> <span class="n">n2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;N1: </span><span class="si">{</span><span class="n">n1</span><span class="si">}</span><span class="s2">; N2: </span><span class="si">{</span><span class="n">n2</span><span class="si">}</span><span class="s2">; N3: </span><span class="si">{</span><span class="n">n3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">compare_systems</span><span class="p">([</span><span class="n">n1</span><span class="p">,</span> <span class="n">n2</span><span class="p">,</span> <span class="n">n3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="mi">1000000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob(System1 &gt; System2) = </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">prob</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Total number of samples: 50
Ratio negatives/positives: 20%
System 1 accuracy: 88.00%
System 2 accuracy: 82.00%
N1: 8; N2: 5; N3: 37
Prob(System1 &gt; System2) = 79.67%
</pre></div>
</div>
</section>
</section>
<section id="q-a">
<h2>Q&amp;A<a class="headerlink" href="#q-a" title="Permalink to this headline">¶</a></h2>
<section id="i-m-confused-what-should-i-choose">
<h3>I’m confused, what should I choose?<a class="headerlink" href="#i-m-confused-what-should-i-choose" title="Permalink to this headline">¶</a></h3>
<p>You normally want to use the Bayesian approach with the Beta prior, which has a
more natural interpretation.  The frequentist interpretation is harder to
grasp.  The Bayesian approach with a Beta prior offers a good coverage that is
not too conservative.</p>
</section>
<section id="what-if-my-sampling-is-not-i-i-d">
<h3>What if my sampling is not i.i.d.?<a class="headerlink" href="#what-if-my-sampling-is-not-i-i-d" title="Permalink to this headline">¶</a></h3>
<p>Then using these estimates is not strictly correct, but often done.  If your
samples are not i.i.d. (e.g. phonemes in a speech sequence, pixels in an
image), then these methods will probably provide an overly optimistic estimate
of the interval (i.e., probably the interval for 95% confidence would be larger
if you considered the sample dependence).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intuition only.  A reference is missing for this.  If you know of anything,
please patch this description.</p>
</div>
</section>
<section id="should-i-consider-the-expected-value-or-mode-of-a-credible-interval">
<h3>Should I consider the expected value or mode of a credible interval?<a class="headerlink" href="#should-i-consider-the-expected-value-or-mode-of-a-credible-interval" title="Permalink to this headline">¶</a></h3>
<p>TL;DR: Use the mode.</p>
<p>As stated in <a class="reference internal" href="references.html#goutte-2005" id="id8"><span>[GOUTTE-2005]</span></a>, Section 2.1, if you are using a flat prior
(<span class="math notranslate nohighlight">\(\lambda = 1\)</span>), then the mode matches the maximum likelihood (ML)
estimate for the indicator in question.  For example, in the case of precision,
the mode of the posterior will be exactly <span class="math notranslate nohighlight">\(TP/(TP+FP)\)</span>.  The mode is also
the reference for the interval: a confidence interval of 95% is split in half
at each side of the mode.</p>
<p>The expected value will be a smoothed version of the ML estimate
<span class="math notranslate nohighlight">\((TP+1)/(TP+FP+2)\)</span> in the case of precision.</p>
</section>
<section id="is-there-a-better-way-to-compare-2-systems">
<h3>Is there a better way to compare 2 systems?<a class="headerlink" href="#is-there-a-better-way-to-compare-2-systems" title="Permalink to this headline">¶</a></h3>
<p>TL;DR: Use our <a class="reference internal" href="references.html#goutte-2005" id="id9"><span>[GOUTTE-2005]</span></a> implementation to compare two systems using the
F1-score, or a paired comparison.</p>
<p>In Machine Learning, one typically wants to compare two (or more) systems when
subject to the same input data (samples).  Interval estimates provided in this
package make no assumptions about the underlying samples used in comparing
different systems.  Therefore, intervals provided by simply applying one of the
techniques described before, may be overly pessimistic <strong>in the condition
systems are subject to the same input samples</strong>.</p>
<p>In the specific case two systems are subject to <strong>the same input samples</strong>, a
paired-test may be more adequate.  Unfortunately, for indicators such as
precision, recall or F1-score, many of the paired-tests available in literature
are not adequate <a class="reference internal" href="references.html#goutte-2005" id="id10"><span>[GOUTTE-2005]</span></a>.</p>
<p>The main reason for the inadequacy lies on the constraints imposed by such
tests (e.g. gaussianity or symmetry).  In a well-trained system, we would
expect positive and negative samples to be closely located to the extremes of
the system output range (e.g. inside the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>).  Besides
being bounded, each of those distributions are likely composed of uni-lateral
tails.  Hence, differences between outputs of systems may not be approximately
normal.  Here is summary of available paired tests and their
assumptions/adequacy:</p>
<ul class="simple">
<li><p>Paired (Student’s) t-test (parametric): the difference between system outputs
should be (<em>approximately</em>) normally distributed, whereas it is likely
to be bi-modal.</p></li>
<li><p>Wilcoxon (signed rank) test (non-parametric): does not assume gaussianity on
the difference of outputs.  It measures how symmetric the difference of
outputs is around the median.  Assuming symmetry for the difference of two
heavily tailed distributions could be quite restricting.</p></li>
<li><p>Sign test (non-parametric): Like Wilcoxon test, but without the assumption of
symmetric distribution of the differences around the median.  This test would
be adequate for ML system comparison, but is less sensitive than the two
precendent ones.</p></li>
<li><p>Bootstrap: A frequentist approach to confidence interval estimation by
sampling with replacement (differences of) indicators one would want to
compare.  This would also be adequate as means to compare two systems.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="guide.html" class="btn btn-neutral float-left" title="User Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api.html" class="btn btn-neutral float-right" title="Python API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Idiap Research Institute.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>