<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>User Guide &mdash; bob.measure 4.2.4b0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="_static/plot_directive.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Credible and Confidence Intervals" href="ci.html" />
    <link rel="prev" title="Performance Measurement" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> bob.measure
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                4.2.4b0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#verification">Verification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#identification">Identification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#plotting">Plotting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#roc">ROC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#det">DET</a></li>
<li class="toctree-l3"><a class="reference internal" href="#epc">EPC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cmc">CMC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detection-identification-curve">Detection &amp; Identification Curve</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tunning">Fine-tunning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#full-applications">Full applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#metrics">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plots">Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluate">Evaluate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ci.html">Credible and Confidence Intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">bob.measure</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>User Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="user-guide">
<span id="bob-measure-user-guide"></span><h1>User Guide<a class="headerlink" href="#user-guide" title="Permalink to this headline">¶</a></h1>
<p>Methods in the <code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure</span></code> module can help you to quickly and easily
evaluate error for multi-class or binary classification problems.  If you are
not yet familiarized with aspects of performance evaluation, we recommend the
papers and book chapters on the <a class="reference internal" href="references.html#bob-measure-references"><span class="std std-ref">References</span></a> section for an
overview of some of the implemented methods.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>A classifier is subject to two types of errors, either the event one wishes to
detect is rejected (false negative) or an the noise or background one wishes to
discard is accepted (false positive). A possible way to measure the detection
performance is to use the half-total error rate (HTER), which combines the
False Negative Rate (FNR) and the False Positive Rate (FPR) and is defined in
the following formula:</p>
<div class="math notranslate nohighlight">
\[HTER(\tau, \mathcal{D}) = \frac{FPR(\tau, \mathcal{D}) + FNR(\tau, \mathcal{D})}{2} \quad \textrm{[\%]}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> denotes the dataset used. Since both the FPR and the
FNR depends on the threshold <span class="math notranslate nohighlight">\(\tau\)</span>, they are strongly related to each
other: increasing the FPR will reduce the FNR and vice-versa. For this reason,
results are often presented using either a Receiver Operating Characteristic
(ROC) or a Detection-Error Tradeoff (DET) plot, these two plots basically
present the FPR versus the FNR for different values of the threshold. Another
widely used measure to summarise the performance of a system is the Equal Error
Rate (EER), defined as the point along the ROC or DET curve where the FPR
equals the FNR.</p>
<p>However, it was noted in by <a class="reference internal" href="references.html#bengio-2004" id="id1"><span>[BENGIO-2004]</span></a> that ROC and DET curves may be
misleading when comparing systems. Hence, the so-called Expected Performance
Curve (EPC) was proposed and consists of an unbiased estimate of the reachable
performance of a system at various operating points.  Indeed, in real-world
scenarios, the threshold <span class="math notranslate nohighlight">\(\tau\)</span> has to be set a priori: this is typically
done using a development set (also called cross-validation set). Nevertheless,
the optimal threshold can be different depending on the relative importance
given to the FPR and the FNR. Hence, in the EPC framework, the cost
<span class="math notranslate nohighlight">\(\beta \in [0;1]\)</span> is defined as the trade-off between the FPR and FNR.
The optimal threshold <span class="math notranslate nohighlight">\(\tau^*\)</span> is then computed using different values of
<span class="math notranslate nohighlight">\(\beta\)</span>, corresponding to different operating points:</p>
<div class="math notranslate nohighlight">
\[\tau^{*} = \arg\!\min_{\tau} \quad \beta \cdot \textrm{FPR}(\tau, \mathcal{D}_{d}) + (1-\beta) \cdot \textrm{FNR}(\tau, \mathcal{D}_{d})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{D}_{d}\)</span> denotes the development set and should be
completely separate to the evaluation set <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>Performance for different values of <span class="math notranslate nohighlight">\(\beta\)</span> is then computed on the
evaluation set <span class="math notranslate nohighlight">\(\mathcal{D}_{t}\)</span> using the previously derived threshold.
Note that setting <span class="math notranslate nohighlight">\(\beta\)</span> to 0.5 yields to the Half Total Error Rate
(HTER) as defined in the first equation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Most of the methods available in this module require as input a set of 2
<a class="reference external" href="https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a> objects that contain the scores obtained by the
classification system to be evaluated, without specific order. Most of the
classes that are defined to deal with two-class problems. Therefore, in this
setting, and throughout this manual, we have defined that the <strong>negatives</strong>
represents the impostor attacks or false class accesses (that is when a
sample of class A is given to the classifier of another class, such as class
B) for of the classifier. The second set, referred as the <strong>positives</strong>
represents the true class accesses or signal response of the classifier. The
vectors are called this way because the procedures implemented in this
module expects that the scores of <strong>negatives</strong> to be statistically
distributed to the left of the signal scores (the <strong>positives</strong>). If that is
not the case, one should either invert the input to the methods or multiply
all scores available by -1, in order to have them inverted.</p>
<p>The input to create these two vectors is generated by experiments conducted
by the user and normally sits in files that may need some parsing before
these vectors can be extracted. While it is not possible to provide a parser
for every individual file that may be generated in different experimental
frameworks, we do provide a parser for a generic two columns format where
the first column contains -1/1 for negative/positive and the second column
contains score values. Please refer to the documentation of
<a class="reference internal" href="api/bob.measure.load.html#bob.measure.load.split" title="bob.measure.load.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.load.split()</span></code></a> for more details.</p>
<p>In the remainder of this section we assume you have successfully parsed and
loaded your scores in two 1D float64 vectors and are ready to evaluate the
performance of the classifier.</p>
</div>
</section>
<section id="verification">
<h2>Verification<a class="headerlink" href="#verification" title="Permalink to this headline">¶</a></h2>
<p>To count the number of correctly classified (true) positives and negatives you
can use the following techniques:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># negatives, positives = parse_my_scores(...) # write parser if not provided!</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">T</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1">#Threshold: later we explain how one can calculate these</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">correct_negatives</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">binary</span><span class="o">.</span><span class="n">true_negatives</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FPR</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">correct_negatives</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">/</span><span class="n">negatives</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">correct_positives</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">binary</span><span class="o">.</span><span class="n">true_positives</span><span class="p">(</span><span class="n">positives</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FNR</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">correct_positives</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">/</span><span class="n">positives</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
<p>We do provide a method to calculate the FPR and FNR in a single shot:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">FPR</span><span class="p">,</span> <span class="n">FNR</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">binary</span><span class="o">.</span><span class="n">fprfnr</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p>The threshold <code class="docutils literal notranslate"><span class="pre">T</span></code> is normally calculated by looking at the distribution of
negatives and positives in a development (or validation) set, selecting a
threshold that matches a certain criterion and applying this derived threshold
to the evaluation set. This technique gives a better overview of the
generalization of a method. We implement different techniques for the
calculation of the threshold:</p>
<ul>
<li><p>Threshold for the EER</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">brute_force</span><span class="o">.</span><span class="n">eer_threshold</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Threshold for the minimum HTER</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">brute_force</span><span class="o">.</span><span class="n">min_hter_threshold</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Threshold for the minimum weighted error rate (MWER) given a certain cost
<span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cost</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1">#or &quot;beta&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">T</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">brute_force</span><span class="o">.</span><span class="n">min_weighted_error_rate_threshold</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">,</span> <span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By setting cost to 0.5 is equivalent to use
<a class="reference internal" href="api/bob.measure.brute_force.html#bob.measure.brute_force.min_hter_threshold" title="bob.measure.brute_force.min_hter_threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.brute_force.min_hter_threshold()</span></code></a>.</p>
</div>
</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Often, it is not numerically possible to match the requested criteria for
calculating the threshold based on the provided scores. Instead, the closest
possible threshold is returned. For example, using
<a class="reference internal" href="api/bob.measure.brute_force.html#bob.measure.brute_force.eer_threshold" title="bob.measure.brute_force.eer_threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.brute_force.eer_threshold()</span></code></a> <strong>will not</strong> give you a
threshold where <span class="math notranslate nohighlight">\(FPR == FNR\)</span>. Hence, you cannot report <span class="math notranslate nohighlight">\(FPR\)</span> or
<span class="math notranslate nohighlight">\(FNR\)</span> instead of <span class="math notranslate nohighlight">\(EER\)</span>; you should report <span class="math notranslate nohighlight">\((FPR+FNR)/2\)</span>.
This is also true for <a class="reference internal" href="api/bob.measure.brute_force.html#bob.measure.brute_force.fpr_threshold" title="bob.measure.brute_force.fpr_threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.brute_force.fpr_threshold()</span></code></a> and
<a class="reference internal" href="api/bob.measure.brute_force.html#bob.measure.brute_force.fnr_threshold" title="bob.measure.brute_force.fnr_threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.brute_force.fnr_threshold()</span></code></a>. The threshold returned by
those functions does not guarantee that using that threshold you will get
the requested <span class="math notranslate nohighlight">\(FPR\)</span> or <span class="math notranslate nohighlight">\(FNR\)</span> value. Instead, you should
recalculate using <a class="reference internal" href="api/bob.measure.binary.html#bob.measure.binary.fprfnr" title="bob.measure.binary.fprfnr"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.binary.fprfnr()</span></code></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many functions in <code class="docutils literal notranslate"><span class="pre">bob.measure</span></code> have an <code class="docutils literal notranslate"><span class="pre">is_sorted</span></code> parameter, which
defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>, throughout.  However, these functions need sorted
<code class="docutils literal notranslate"><span class="pre">positive</span></code> and/or <code class="docutils literal notranslate"><span class="pre">negative</span></code> scores.  If scores are not in ascendantly
sorted order, internally, they will be copied – twice!  To avoid scores to
be copied, you might want to sort the scores in ascending order, e.g., by:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">negatives</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">positives</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">brute_force</span><span class="o">.</span><span class="n">min_weighted_error_rate_threshold</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">is_sorted</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">T</span> <span class="o">==</span> <span class="n">t</span>
</pre></div>
</div>
</div>
</section>
<section id="identification">
<h2>Identification<a class="headerlink" href="#identification" title="Permalink to this headline">¶</a></h2>
<p>For identification, the Recognition Rate is one of the standard measures.  To
compute recognition rates, you can use the
<a class="reference internal" href="api/bob.measure.cmc.html#bob.measure.cmc.recognition_rate" title="bob.measure.cmc.recognition_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.cmc.recognition_rate()</span></code></a> function.  This function expects a
relatively complex data structure, which is the same as for the <a class="reference internal" href="#cmc">CMC</a> below.
For each probe item, the scores for negative and positive comparisons are
computed, and collected for all probe items:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rr_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">probe</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">pos</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">neg</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">19</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">rr_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">neg</span><span class="p">,</span> <span class="n">pos</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rr</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">cmc</span><span class="o">.</span><span class="n">recognition_rate</span><span class="p">(</span><span class="n">rr_scores</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>For open set identification, according to Li and Jain (2005) there are two
different error measures defined.  The first measure is the
<a class="reference internal" href="api/bob.measure.cmc.html#bob.measure.cmc.detection_identification_rate" title="bob.measure.cmc.detection_identification_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.cmc.detection_identification_rate()</span></code></a>, which counts the
number of correctly classified in-gallery probe items.  The second measure is
the <a class="reference internal" href="api/bob.measure.cmc.html#bob.measure.cmc.false_alarm_rate" title="bob.measure.cmc.false_alarm_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.cmc.false_alarm_rate()</span></code></a>, which counts, how often an
out-of-gallery probe item was incorrectly accepted.  Both rates can be computed
using the same data structure, with one exception.  Both functions require that
at least one probe item exists, which has no according gallery item, i.e.,
where the positives are empty or <code class="docutils literal notranslate"><span class="pre">None</span></code>:</p>
<p>(continued from above…)</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">probe</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">pos</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">... </span>  <span class="n">neg</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>  <span class="n">rr_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">neg</span><span class="p">,</span> <span class="n">pos</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">dir</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">cmc</span><span class="o">.</span><span class="n">detection_identification_rate</span><span class="p">(</span><span class="n">rr_scores</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span> <span class="o">=</span> <span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">cmc</span><span class="o">.</span><span class="n">false_alarm_rate</span><span class="p">(</span><span class="n">rr_scores</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plotting">
<h2>Plotting<a class="headerlink" href="#plotting" title="Permalink to this headline">¶</a></h2>
<p>An image is worth 1000 words, they say. You can combine the capabilities of
<a class="reference external" href="https://matplotlib.org/index.html#module-matplotlib" title="(in Matplotlib v3.5.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">matplotlib</span></code></a> with functions in this package to plot a number of curves.
However, you must have that package installed.  In this section we describe a
few recipes.</p>
<section id="roc">
<h3>ROC<a class="headerlink" href="#roc" title="Permalink to this headline">¶</a></h3>
<p>The Receiver Operating Characteristic (ROC) curve is one of the oldest plots in
town. To plot an ROC curve, in possession of your <strong>negatives</strong> and
<strong>positives</strong>, just do something along the lines of:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we assume you have your negatives and positives already split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npoints</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">roc</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">,</span> <span class="n">npoints</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;FPR (%)&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FNR (%)&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># You can also compute the area under the ROC curve:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">curves</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">)</span>
<span class="go">0.9169...</span>
</pre></div>
</div>
<p>You should see an image like the following one:</p>
<p>(<a class="reference external" href=".//guide-1.py">Source code</a>, <a class="reference external" href=".//guide-1.png">png</a>, <a class="reference external" href=".//guide-1.hires.png">hires.png</a>, <a class="reference external" href=".//guide-1.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/guide-1.png" class="plot-directive" src="_images/guide-1.png" />
</figure>
<p>As can be observed, plotting methods live in the namespace
<a class="reference internal" href="api/bob.measure.plot.html#module-bob.measure.plot" title="bob.measure.plot"><code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure.plot</span></code></a>. They work like the
<a class="reference external" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" title="(in Matplotlib v3.5.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">matplotlib.pyplot.plot()</span></code></a> itself, except that instead of receiving the
x and y point coordinates as parameters, they receive the two
<a class="reference external" href="https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a> arrays with negatives and positives, as well as an
indication of the number of points the curve must contain.</p>
<p>As in the <a class="reference external" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" title="(in Matplotlib v3.5.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">matplotlib.pyplot.plot()</span></code></a> command, you can pass optional
parameters for the line as shown in the example to setup its color, shape and
even the label.  For an overview of the keywords accepted, please refer to the
<a class="reference external" href="https://matplotlib.org/index.html#module-matplotlib" title="(in Matplotlib v3.5.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">matplotlib</span></code></a> Documentation. Other plot properties such as the plot title,
axis labels, grids, legends should be controlled directly using the relevant
<a class="reference external" href="https://matplotlib.org/index.html#module-matplotlib" title="(in Matplotlib v3.5.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">matplotlib</span></code></a>’s controls.</p>
</section>
<section id="det">
<h3>DET<a class="headerlink" href="#det" title="Permalink to this headline">¶</a></h3>
<p>A DET curve can be drawn using similar commands such as the ones for the ROC
curve:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we assume you have your negatives and positives already split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">npoints</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">negatives</span><span class="p">,</span> <span class="n">positives</span><span class="p">,</span> <span class="n">npoints</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">det_axis</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;FPR (%)&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FNR (%)&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
<p>This will produce an image like the following one:</p>
<p>(<a class="reference external" href=".//guide-2.py">Source code</a>, <a class="reference external" href=".//guide-2.png">png</a>, <a class="reference external" href=".//guide-2.hires.png">hires.png</a>, <a class="reference external" href=".//guide-2.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/guide-2.png" class="plot-directive" src="_images/guide-2.png" />
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you wish to reset axis zooming, you must use the Gaussian scale rather
than the visual marks showed at the plot, which are just there for displaying
purposes. The real axis scale is based on the
<a class="reference internal" href="api/bob.measure.curves.html#bob.measure.curves.ppndf" title="bob.measure.curves.ppndf"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.curves.ppndf()</span></code></a> method. For example, if you wish to set
the x and y axis to display data between 1% and 40% here is the recipe:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#AFTER you plot the DET curve, just set the axis in this way:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">curves</span><span class="o">.</span><span class="n">ppndf</span><span class="p">(</span><span class="n">k</span><span class="o">/</span><span class="mf">100.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">)])</span> 
</pre></div>
</div>
<p>We provide a convenient way for you to do the above in this module. So,
optionally, you may use the <code class="docutils literal notranslate"><span class="pre">bob.measure.plot.det_axis</span></code> method like this:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">det_axis</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span> 
</pre></div>
</div>
</div>
</section>
<section id="epc">
<h3>EPC<a class="headerlink" href="#epc" title="Permalink to this headline">¶</a></h3>
<p>Drawing an EPC requires that both the development set negatives and positives
are provided alongside the evaluation set ones. Because of this the API is
slightly modified:</p>
<div class="highlight-pycon3 notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bob</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">epc</span><span class="p">(</span><span class="n">dev_neg</span><span class="p">,</span> <span class="n">dev_pos</span><span class="p">,</span> <span class="n">test_neg</span><span class="p">,</span> <span class="n">test_pos</span><span class="p">,</span> <span class="n">npoints</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
<p>This will produce an image like the following one:</p>
<p>(<a class="reference external" href=".//guide-3.py">Source code</a>, <a class="reference external" href=".//guide-3.png">png</a>, <a class="reference external" href=".//guide-3.hires.png">hires.png</a>, <a class="reference external" href=".//guide-3.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/guide-3.png" class="plot-directive" src="_images/guide-3.png" />
</figure>
</section>
<section id="cmc">
<h3>CMC<a class="headerlink" href="#cmc" title="Permalink to this headline">¶</a></h3>
<p>The Cumulative Match Characteristics (CMC) curve estimates the probability that
the correct model is in the <em>N</em> models with the highest similarity to a given
probe.  A CMC curve can be plotted using the <a class="reference internal" href="api/bob.measure.plot.html#bob.measure.plot.cmc" title="bob.measure.plot.cmc"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.plot.cmc()</span></code></a>
function.  The CMC can be calculated from a relatively complex data structure,
which defines a pair of positive and negative scores <strong>per probe</strong>:</p>
<p>(<a class="reference external" href=".//guide-4.py">Source code</a>, <a class="reference external" href=".//guide-4.png">png</a>, <a class="reference external" href=".//guide-4.hires.png">hires.png</a>, <a class="reference external" href=".//guide-4.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/guide-4.png" class="plot-directive" src="_images/guide-4.png" />
</figure>
<p>Usually, there is only a single positive score per probe, but this is not a fixed restriction.</p>
</section>
<section id="detection-identification-curve">
<h3>Detection &amp; Identification Curve<a class="headerlink" href="#detection-identification-curve" title="Permalink to this headline">¶</a></h3>
<p>The detection &amp; identification curve is designed to evaluate open set
identification tasks.  It can be plotted using the
<a class="reference internal" href="api/bob.measure.plot.html#bob.measure.plot.detection_identification_curve" title="bob.measure.plot.detection_identification_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.plot.detection_identification_curve()</span></code></a> function, but it
requires at least one open-set probe, i.e., where no corresponding positive
score exists, for which the FPR values are computed.  Here, we plot the
detection and identification curve for rank 1, so that the recognition rate for
FPR=1 will be identical to the rank one <code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.recognition_rate()</span></code>
obtained in the CMC plot above.</p>
<p>(<a class="reference external" href=".//guide-5.py">Source code</a>, <a class="reference external" href=".//guide-5.png">png</a>, <a class="reference external" href=".//guide-5.hires.png">hires.png</a>, <a class="reference external" href=".//guide-5.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/guide-5.png" class="plot-directive" src="_images/guide-5.png" />
</figure>
</section>
<section id="fine-tunning">
<h3>Fine-tunning<a class="headerlink" href="#fine-tunning" title="Permalink to this headline">¶</a></h3>
<p>The methods inside <a class="reference internal" href="api/bob.measure.plot.html#module-bob.measure.plot" title="bob.measure.plot"><code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure.plot</span></code></a> are only provided as a
<a class="reference external" href="https://matplotlib.org/index.html#module-matplotlib" title="(in Matplotlib v3.5.1)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">matplotlib</span></code></a> wrapper to equivalent methods in <code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure</span></code>
that can only calculate the points without doing any plotting. You may prefer
to tweak the plotting or even use a different plotting system such as gnuplot.
Have a look at the implementations at <a class="reference internal" href="api/bob.measure.plot.html#module-bob.measure.plot" title="bob.measure.plot"><code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure.plot</span></code></a> to understand
how to use the Bob methods to compute the curves and interlace that in
the way that best suits you.</p>
</section>
</section>
<section id="full-applications">
<span id="bob-measure-command-line"></span><h2>Full applications<a class="headerlink" href="#full-applications" title="Permalink to this headline">¶</a></h2>
<p>Commands under <code class="docutils literal notranslate"><span class="pre">bob</span> <span class="pre">measure</span></code> can be used to quickly evaluate a set of
scores and generate plots. We present these commands in this section. The commands
take as input generic 2-column data format as specified in the documentation of
<a class="reference internal" href="api/bob.measure.load.html#bob.measure.load.split" title="bob.measure.load.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">bob.measure.load.split()</span></code></a></p>
<section id="metrics">
<h3>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">¶</a></h3>
<p>To calculate the threshold using a certain criterion (EER (default) or min.HTER)
on a development set and conduct the threshold computation and its performance
on an evaluation set, after setting up Bob, just do:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>./bin/bob measure  metrics ./MTest1/scores-<span class="o">{</span>dev,eval<span class="o">}</span> -e -v
<span class="o">[</span>Min. criterion: EER <span class="o">]</span> Threshold on Development <span class="nb">set</span> <span class="sb">`</span>./MTest1/scores-dev<span class="sb">`</span>: -1.373550e-02
bob.measure@2018-06-29 <span class="m">10</span>:20:14,177 -- WARNING: NaNs scores <span class="o">(</span><span class="m">1</span>.0%<span class="o">)</span> were found <span class="k">in</span> ./MTest1/scores-dev
bob.measure@2018-06-29 <span class="m">10</span>:20:14,177 -- WARNING: NaNs scores <span class="o">(</span><span class="m">1</span>.0%<span class="o">)</span> were found <span class="k">in</span> ./MTest1/scores-eval
<span class="o">===================</span>  <span class="o">================</span>  <span class="o">================</span>
..                   Development       <span class="nv">Evaluation</span>
<span class="o">===================</span>  <span class="o">================</span>  <span class="o">================</span>
False Positive Rate  <span class="m">15</span>.5% <span class="o">(</span><span class="m">767</span>/4942<span class="o">)</span>  <span class="m">15</span>.5% <span class="o">(</span><span class="m">767</span>/4942<span class="o">)</span>
False Negative Rate  <span class="m">15</span>.5% <span class="o">(</span><span class="m">769</span>/4954<span class="o">)</span>  <span class="m">15</span>.5% <span class="o">(</span><span class="m">769</span>/4954<span class="o">)</span>
Precision            <span class="m">0</span>.8               <span class="m">0</span>.8
Recall               <span class="m">0</span>.8               <span class="m">0</span>.8
F1-score             <span class="m">0</span>.8               <span class="m">0</span>.8
<span class="o">===================</span>  <span class="o">================</span>  <span class="o">================</span>
</pre></div>
</div>
<p>The output will present the threshold together with the FPR, FNR, Precision,
Recall, F1-score and HTER on the given set, calculated using such a threshold.
The relative counts of FAs and FRs are also displayed between parenthesis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Several scores files can be given at once and the metrics will be computed
for each of them separatly. Development and evaluation files must be given
by pairs. When evaluation files are provided, <code class="docutils literal notranslate"><span class="pre">--eval</span></code> flag must be given.</p>
</div>
<p>To evaluate the performance of a new score file with a given threshold, use
<code class="docutils literal notranslate"><span class="pre">--thres</span></code>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>./bin/bob measure  metrics ./MTest1/scores-eval -v --thres <span class="m">0</span>.006
<span class="o">[</span>Min. criterion: user provided<span class="o">]</span> Threshold on Development <span class="nb">set</span> <span class="sb">`</span>./MTest1/scores-eval<span class="sb">`</span>: <span class="m">6</span>.000000e-03
bob.measure@2018-06-29 <span class="m">10</span>:22:06,852 -- WARNING: NaNs scores <span class="o">(</span><span class="m">1</span>.0%<span class="o">)</span> were found <span class="k">in</span> ./MTest1/scores-eval
<span class="o">===================</span>  <span class="o">================</span>
..                   <span class="nv">Development</span>
<span class="o">===================</span>  <span class="o">================</span>
False Positive Rate  <span class="m">15</span>.2% <span class="o">(</span><span class="m">751</span>/4942<span class="o">)</span>
False Negative Rate  <span class="m">16</span>.1% <span class="o">(</span><span class="m">796</span>/4954<span class="o">)</span>
Precision            <span class="m">0</span>.8
Recall               <span class="m">0</span>.8
F1-score             <span class="m">0</span>.8
<span class="o">===================</span>  <span class="o">================</span>
</pre></div>
</div>
<p>You can simultaneously conduct the threshold computation and its performance on
an evaluation set:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Table format can be changed using <code class="docutils literal notranslate"><span class="pre">--tablefmt</span></code> option, the default format
being <code class="docutils literal notranslate"><span class="pre">rst</span></code>. Please refer to <code class="docutils literal notranslate"><span class="pre">bob</span> <span class="pre">measure</span> <span class="pre">metrics</span> <span class="pre">--help</span></code> for more
details.</p>
</div>
</section>
<section id="plots">
<h3>Plots<a class="headerlink" href="#plots" title="Permalink to this headline">¶</a></h3>
<p>Customizable plotting commands are available in the <code class="xref py py-mod docutils literal notranslate"><span class="pre">bob.measure</span></code>
module.  They take a list of development and/or evaluation files and generate a
single PDF file containing the plots. Available plots are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">roc</span></code> (receiver operating characteristic)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">det</span></code> (detection error trade-off)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epc</span></code> (expected performance curve)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hist</span></code> (histograms of positive and negatives)</p></li>
</ul>
<p>Use the <code class="docutils literal notranslate"><span class="pre">--help</span></code> option on the above-cited commands to find-out about more
options.</p>
<p>For example, to generate a DET curve from development and evaluation datasets:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">$bob</span> measure det -e -v --output <span class="s2">&quot;my_det.pdf&quot;</span> -ts <span class="s2">&quot;DetDev1,DetEval1,DetDev2,DetEval2&quot;</span>
dev-1.txt eval-1.txt dev-2.txt eval-2.txt
</pre></div>
</div>
<p>where <cite>my_det.pdf</cite> will contain DET plots for the two experiments.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">det</span></code> and <code class="docutils literal notranslate"><span class="pre">roc</span></code> plot development and evaluation curves on
different plots. You can force gather everything in the same plot using
<code class="docutils literal notranslate"><span class="pre">--no-split</span></code> option.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--figsize</span></code> and <code class="docutils literal notranslate"><span class="pre">--style</span></code> options are two powerful options that can
dramatically change the appearance of your figures. Try them! (e.g.
<code class="docutils literal notranslate"><span class="pre">--figsize</span> <span class="pre">12,10</span> <span class="pre">--style</span> <span class="pre">grayscale</span></code>)</p>
</div>
</section>
<section id="evaluate">
<h3>Evaluate<a class="headerlink" href="#evaluate" title="Permalink to this headline">¶</a></h3>
<p>A convenient command <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> is provided to generate multiple metrics and
plots for a list of experiments. It generates two <code class="docutils literal notranslate"><span class="pre">metrics</span></code> outputs with ERR
and min-HTER criteria along with <code class="docutils literal notranslate"><span class="pre">roc</span></code>, <code class="docutils literal notranslate"><span class="pre">det</span></code>, <code class="docutils literal notranslate"><span class="pre">epc</span></code>, <code class="docutils literal notranslate"><span class="pre">hist</span></code> plots for
each experiment. For example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">$bob</span> measure evaluate -e -v -l <span class="s1">&#39;my_metrics.txt&#39;</span> -o <span class="s1">&#39;my_plots.pdf&#39;</span> <span class="o">{</span>sys1,sys2<span class="o">}</span>/<span class="o">{</span>dev,eval<span class="o">}</span>
</pre></div>
</div>
<p>will output metrics and plots for the two experiments (dev and eval pairs) in
<code class="docutils literal notranslate"><span class="pre">my_metrics.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">my_plots.pdf</span></code>, respectively.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Performance Measurement" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ci.html" class="btn btn-neutral float-right" title="Credible and Confidence Intervals" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Idiap Research Institute.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>